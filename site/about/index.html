<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://kimit0310.github.io/MoBI_Docs/about/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>About - MoBI Setup Documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "About";
        var mkdocs_page_input_path = "about.md";
        var mkdocs_page_url = "/MoBI_Docs/about/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> MoBI Setup Documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">About</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#methods">Methods</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#results">Results</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#conclusions">Conclusions</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#references">References</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../computing/">Computing</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../EEG/">EEG</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../EyeTracking_Wearable/">Wearable Eye Tracking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../EyeTracking_Tabletop/">Tabletop Eye Tracking</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Physiology/">Electrophysiology</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Audio/">Audio</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Video/">Video</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../MotionCapture/">Motion Capture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../3D/">3D Scanning</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../Cogpsych/">Cogpsych</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../appendices/">Appendices</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">MoBI Setup Documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">About</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="the-next-generation-of-data-collection-is-multimodal">The Next Generation of Data Collection is Multimodal</h1>
<p>Nathalia Bianchini Esper<sup>1</sup>, Adam Santorelli<sup>1</sup>, Bryan Gonzalez<sup>1</sup>, Nicole Burke<sup>1</sup>, Samuel Louviot<sup>1</sup>, Alp Erkent<sup>1</sup>, Apurva Gokhe<sup>1</sup>, Camilla Strauss<sup>1</sup>, Celia Maiorano<sup>1</sup>, Iktae Kim<sup>1</sup>, Freymon Perez<sup>1</sup>, John Vito d’Antonio-Bertagnolli<sup>1</sup>, Stan Colcombe<sup>1</sup> <sup>2</sup>, Alexandre Rosa Franco<sup>1</sup> <sup>2</sup>, Gregory Kiar<sup>1</sup>, Michelle Freund<sup>1</sup>, Michael P. Milham<sup>1</sup> <sup>2</sup></p>
<p><sup>1</sup> Child Mind Institute, New York, USA<br />
<sup>2</sup> Nathan S. Kline Institute for Psychiatric Research, New York, USA  </p>
<hr />
<h2 id="introduction">Introduction</h2>
<p>With the continuous refinement of brain imaging methods such as MRI and EEG, researchers are gaining better insights into brain structure, function, and connectivity, enabling advances in understanding neurological and psychiatric disorders, as well as the neural basis of cognition, behavior, and emotion (Warbrick, 2022). A multimodal approach allows researchers to integrate complementary data types, addressing limitations of single-modality experiments and providing a broader perspective on brain function, connectivity, facial expressions, body language, and environmental context (Calhoun &amp; Sui, 2016; Wagner et al., 2019). This approach is particularly valuable in social and cognitive sciences, offering deeper insights into communication, emotion regulation, and social interactions (Madsen &amp; Parra, 2024). </p>
<p>Here, we present a laboratory design for the next generation of data collection: a multimodal brain/body imaging approach (MoBI). A MoBI laboratory (Makeig et al., 2009) uses various techniques and data sources to examine brain activity in dynamic, interactive scenarios, along with other physiological and behavioral measures, including EEG, eye-tracking, motion capture, electromyography (EMG), electrocardiography (ECG), galvanic skin response, and audio/video recordings.</p>
<hr />
<h2 id="methods">Methods</h2>
<p>Multimodal data collection presents two primary challenges:</p>
<ol>
<li><strong>Hardware and Software Requirements</strong>: Each modality's hardware and software requirements often rely on multiple computing systems and peripheral devices, creating logistical and operational burdens.</li>
<li><strong>Synchronization</strong>: The independent acquisition of data streams for each modality results in separate files, complicating synchronization of timestamps across devices.</li>
</ol>
<p>To overcome these challenges, we developed a centralized hub and data collection system for MoBI setups. This approach consolidates data streams from multiple modalities onto a single computer using the Lab Streaming Layer (LSL) framework (Kothe et al., 2024). By leveraging a shared system clock, we achieve precise synchronization and effective correction for time drift across devices, eliminating the need for additional synchronization hardware or extensive post hoc adjustments. </p>
<p>An integrated architecture reduces latency and hardware complexity, ensures real-time monitoring, and maintains the temporal fidelity of multimodal datasets. To support this implementation, we developed comprehensive documentation that provides detailed guidance on every step of the process, from equipment evaluation through data collection. The documentation includes:</p>
<ul>
<li>Hardware specifications for building a MoBI system.</li>
<li>Configuration details for the centralized computer.</li>
<li>Best practices for data acquisition.</li>
</ul>
<p>This documentation aims to streamline the process for researchers and facilitate the adoption of robust and synchronized multimodal data collection workflows.</p>
<hr />
<h2 id="results">Results</h2>
<p>We have successfully integrated a range of data acquisition systems into our MoBI framework, including (but not limited to):</p>
<ul>
<li>Dry and semi-dry EEG systems.</li>
<li>Table-top and wearable eye-tracking devices.</li>
<li>Physiological measurement modalities such as electrocardiography, electrodermal activity, inductive respiration, and peripheral oxygen saturation.</li>
</ul>
<p>Additionally, the setup supports:</p>
<ul>
<li>Multiple audio and video streams.</li>
<li>Motion capture systems.</li>
<li>Cognitive task presentations using PsychoPy and MindLogger (Klein et al., 2021).</li>
</ul>
<p><center><img src="../img/Computing/5.png" width='700px'></center></p>
<p><strong>Figure 1</strong> shows a schema of our setup. By centralizing computing resources to a single device, we exhibit a cost reduction of up to 50% compared to traditional laboratory designs.</p>
<hr />
<h2 id="conclusions">Conclusions</h2>
<p>Preliminary testing has demonstrated the LSL framework's robust capability to achieve clock synchronization and time drift correction across diverse devices, ensuring accurate temporal alignment of multimodal datasets. Our detailed documentation, publicly hosted at <a href="https://childmindresearch.github.io/MoBI_Docs">childmindresearch.github.io/MoBI_Docs</a>, provides the research community with a valuable resource for replicating and adapting these methodologies for multimodal studies.</p>
<hr />
<h2 id="references">References</h2>
<ul>
<li>Calhoun, V. D., &amp; Sui, J. (2016). Multimodal fusion of brain imaging data: A key to finding the missing link(s) in complex mental illness. <em>Biological Psychiatry: Cognitive Neuroscience and Neuroimaging, 1</em>(3), 230–244. https://doi.org/10.1016/j.bpsc.2015.12.005</li>
<li>Klein, A., et al. (2021). Remote Digital Psychiatry for Mobile Mental Health Assessment and Therapy: MindLogger Platform Development Study. <em>J Med Internet Res, 23</em>(11), e22369. https://doi.org/10.2196/22369</li>
<li>Kothe, C., et al. (2024). The Lab Streaming Layer for synchronized multimodal recording. <em>bioRxiv</em>. https://doi.org/10.1101/2024.02.13.580071</li>
<li>Madsen, J., &amp; Parra, L. C. (2024). Bidirectional brain-body interactions during natural story listening. <em>Cell Reports, 43</em>(4), 114081. https://doi.org/10.1016/j.celrep.2024.114081</li>
<li>Makeig, S., et al. (2009). Linking brain, mind and behavior. <em>International Journal of Psychophysiology, 73</em>(2), 95–100. https://doi.org/10.1016/j.ijpsycho.2008.11.008</li>
<li>Wagner, J., et al. (2019). High-density EEG mobile brain/body imaging data recorded during a challenging auditory gait pacing task. <em>Scientific Data, 6</em>(1), 211. https://doi.org/10.1038/s41597-019-0223-2</li>
<li>Warbrick, T. (2022). Simultaneous EEG-fMRI: What have we learned and what does the future hold? <em>Sensors, 22</em>(6), 2262. https://doi.org/10.3390/s22062262</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href=".." class="btn btn-neutral float-left" title="Home"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../computing/" class="btn btn-neutral float-right" title="Computing">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href=".." style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../computing/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
